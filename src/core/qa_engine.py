import logging
from typing import Dict
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from src.utils.config import OPENAI_API_KEY
from src.data.vector_store import VectorStore

logger = logging.getLogger(__name__)

class QAEngine:
    """Handles document-based question answering with conversation memory"""
    
    def __init__(self, vector_store: VectorStore):
        self.vector_store = vector_store
        self.llm = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.7,
            openai_api_key=OPENAI_API_KEY,
            request_timeout=60
        )
        
        self.prompt = ChatPromptTemplate.from_template("""
        Eres un asistente tÃ©cnico amigable que SIEMPRE responde en espaÃ±ol. 
        Tu personalidad es profesional pero cercana, y te gusta explicar las cosas 
        de manera clara y con ejemplos cuando es posible.

        Directrices:
        - SIEMPRE responde en espaÃ±ol
        - Si se menciona cÃ³digo en el contexto, inclÃºyelo en la explicaciÃ³n
        - Explica los conceptos tÃ©cnicos de forma clara pero precisa
        - Si algo no estÃ¡ en el contexto, dilo honestamente
        - Usa terminologÃ­a tÃ©cnica apropiadamente
        - Si es relevante, menciona quÃ© parte de la documentaciÃ³n contiene la informaciÃ³n
        - SÃ© amigable y cercano, pero mantÃ©n la profesionalidad

        Contexto:
        {context}
        
        Pregunta: {question}
        """)
        
        self.qa_chain = (
            {
                "context": lambda x: self._get_context(x["question"]),
                "question": RunnablePassthrough()
            }
            | self.prompt 
            | self.llm 
            | StrOutputParser()
        )
        
    def _get_context(self, question: str) -> str:
        """Get context from vector store with error handling"""
        try:
            results = self.vector_store.search(question)
            return "\n".join([chunk["chunk"] for chunk in results])
        except Exception as e:
            logger.error(f"Error getting context: {e}")
            return ""

    def get_answer(self, question: str) -> Dict[str, str]:
        """Get answer with error handling and logging"""
        try:
            logger.info(f"Processing question: {question}")
            answer = self.qa_chain.invoke({"question": question})
            return {"answer": answer}
        except Exception as e:
            logger.error(f"Error getting answer: {e}")
            return {
                "error": "Lo siento, hubo un error procesando tu pregunta. Por favor, intenta de nuevo."
            }

    def get_initial_message(self) -> str:
        """Returns the initial greeting message"""
        return """Â¡Hola! ğŸ‘‹ Soy tu asistente tÃ©cnico personal. 
        
Estoy aquÃ­ para ayudarte a entender documentaciÃ³n sobre desarrollo de software, inteligencia artificial, bases de datos, frameworks, arquitectura de sistemas y tecnologÃ­as relacionadas.


CaracterÃ­sticas:
- Memoria persistente: Consulto todos los documentos subidos anteriormente
- AnÃ¡lisis contextual: Respuestas basadas en tu documentaciÃ³n
- Ejemplos prÃ¡cticos cuando sea posible

Para empezar:
1. Sube un documento PDF tÃ©cnico en el panel lateral
2. Hazme preguntas sobre cualquier documento
3. Â¡Te ayudarÃ© a entenderlo!

Â¿En quÃ© puedo ayudarte hoy?"""